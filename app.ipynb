{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on word2vec ... idx  10000\n",
      "working on word2vec ... idx  20000\n",
      "working on word2vec ... idx  30000\n",
      "working on word2vec ... idx  40000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import codecs\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import os.path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Lambda, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from numpy import inf\n",
    "from operator import itemgetter\n",
    "\n",
    "seed = 28\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "top_freq_word_to_use = 40000\n",
    "embedding_dimension = 300\n",
    "max_len_head = 25\n",
    "max_len_desc = 50\n",
    "max_length = max_len_head + max_len_desc\n",
    "rnn_layers = 4\n",
    "rnn_size = 600\n",
    "# first 40 numebers from hidden layer output used for\n",
    "# simple context calculation\n",
    "activation_rnn_size = 50\n",
    "\n",
    "empty_tag_location = 0\n",
    "eos_tag_location = 1\n",
    "unknown_tag_location = 2\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#minimum headline should be genrated\n",
    "min_head_line_gen = 10\n",
    "dont_repeat_word_in_last = 5\n",
    "\n",
    "word2vec = []\n",
    "idx2word = {}\n",
    "word2idx = {}\n",
    "# initalize end of sentence, empty and unk tokens\n",
    "word2idx['<empty>'] = empty_tag_location\n",
    "word2idx['<eos>'] = eos_tag_location\n",
    "word2idx['<unk>'] = unknown_tag_location\n",
    "idx2word[empty_tag_location] = '<empty>'\n",
    "idx2word[eos_tag_location] = '<eos>'\n",
    "idx2word[unknown_tag_location] = '<unk>'\n",
    "\n",
    "idx = 3\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('F:/VIGHNESH/Cognitive_Computing/Final_Project/GoogleNews-vectors-negative300.bin', binary = True, limit = 40000)\n",
    "V = model.index2word\n",
    "for index, word in enumerate(V):\n",
    "    vector = model[word]\n",
    "    word2idx[word] = idx\n",
    "    idx2word[idx] = word\n",
    "    idx = idx + 1\n",
    "    if idx % 10000 == 0:\n",
    "        print (\"working on word2vec ... idx \", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_context(X, mask):\n",
    "    \"\"\"\n",
    "    Simple context calculation layer logic\n",
    "    X = (batch_size, time_steps, units)\n",
    "    time_steps are nothing but number of words in our case.\n",
    "    \"\"\"\n",
    "    # segregrate heading and desc\n",
    "    desc, head = X[:, :max_len_desc, :], X[:, max_len_desc:, :]\n",
    "    # segregrate activation and context part\n",
    "    head_activations, head_words = head[:, :, :activation_rnn_size], head[:, :, activation_rnn_size:]\n",
    "    desc_activations, desc_words = desc[:, :, :activation_rnn_size], desc[:, :, activation_rnn_size:]\n",
    "\n",
    "    # p=(bacth_size, length_desc_words, rnn_units)\n",
    "    # q=(bacth_size, length_headline_words, rnn_units)\n",
    "    # K.dot(p,q) = (bacth_size, length_desc_words,length_headline_words)\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2, 2))\n",
    "\n",
    "    # make sure we dont use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20 * K.expand_dims(1. - K.cast(mask[:, :max_len_desc], 'float32'), 1)\n",
    "\n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies, (-1, max_len_desc))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights, (-1, max_len_head, max_len_desc))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2, 1))\n",
    "    return K.concatenate((desc_avg_word, head_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_shape_simple_context_layer(input_shape):\n",
    "    \"\"\"\n",
    "    Take input shape tuple and return tuple for output shape\n",
    "    Output shape size for simple context layer =\n",
    "    remaining part after activatoion calculation fron input layers avg. +\n",
    "    remaining part after activatoion calculation fron current hidden layers avg.\n",
    "    that is 2 * (rnn_size - activation_rnn_size))\n",
    "    input_shape[0] = batch_size remains as it is\n",
    "    max_len_head = heading max length allowed\n",
    "    \"\"\"\n",
    "    return (input_shape[0], max_len_head , 2 * (rnn_size - activation_rnn_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "        \"\"\"\n",
    "        RNN model creation\n",
    "        Layers include Embedding Layer, 3 LSTM stacked,\n",
    "        Simple Context layer (manually defined),\n",
    "        Time Distributed Layer\n",
    "        \"\"\"\n",
    "        #length_vocab, embedding_size = word2vec.shape\n",
    "        #print (\"shape of word2vec matrix \", word2vec.shape)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # TODO: look at mask zero flag\n",
    "        model.add(\n",
    "                Embedding(\n",
    "                        40003, embedding_dimension,\n",
    "                        input_length=max_length,\n",
    "                         mask_zero=True,\n",
    "                        name='embedding_layer'\n",
    "                )\n",
    "        )\n",
    "\n",
    "        for i in range(rnn_layers):\n",
    "            lstm = LSTM(rnn_size, return_sequences=True,\n",
    "                name='lstm_layer_%d' % (i + 1)\n",
    "            )\n",
    "\n",
    "            model.add(lstm)\n",
    "            # No drop out added !\n",
    "\n",
    "        model.add(Lambda(simple_context,\n",
    "                     mask=lambda inputs, mask: mask[:, max_len_desc:],\n",
    "                     output_shape=output_shape_simple_context_layer,\n",
    "                     name='simple_context_layer'))\n",
    "\n",
    "        vocab_size = 40003\n",
    "        model.add(TimeDistributed(Dense(vocab_size,\n",
    "                                name='time_distributed_layer')))\n",
    "        \n",
    "        model.add(Activation('softmax', name='activation_layer'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        K.set_value(model.optimizer.lr, np.float32(learning_rate))\n",
    "        print (model.summary())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 75, 300)           12000900  \n",
      "_________________________________________________________________\n",
      "lstm_layer_1 (LSTM)          (None, 75, 600)           2162400   \n",
      "_________________________________________________________________\n",
      "lstm_layer_2 (LSTM)          (None, 75, 600)           2882400   \n",
      "_________________________________________________________________\n",
      "lstm_layer_3 (LSTM)          (None, 75, 600)           2882400   \n",
      "_________________________________________________________________\n",
      "lstm_layer_4 (LSTM)          (None, 75, 600)           2882400   \n",
      "_________________________________________________________________\n",
      "simple_context_layer (Lambda (None, 25, 1100)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 25, 40003)         44043303  \n",
      "_________________________________________________________________\n",
      "activation_layer (Activation (None, 25, 40003)         0         \n",
      "=================================================================\n",
      "Total params: 66,853,803\n",
      "Trainable params: 66,853,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights('F:/VIGHNESH/Cognitive_Computing/Final_Project/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(list_idx, curr_max_length, is_left):\n",
    "    \"\"\"\n",
    "    padds with <empty> tag in left side\n",
    "    \"\"\"\n",
    "    print(\"Padding\")\n",
    "    if len(list_idx) >= curr_max_length:\n",
    "        return list_idx\n",
    "    number_of_empty_fill = curr_max_length - len(list_idx)\n",
    "    if is_left:\n",
    "        return [empty_tag_location, ] * number_of_empty_fill + list_idx\n",
    "    else:\n",
    "        return list_idx + [empty_tag_location, ] * number_of_empty_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def headline2idx(list_idx, curr_max_length, is_input):\n",
    "    \"\"\"\n",
    "    if space add <eos> tag in input case, input size = curr_max_length-1\n",
    "    always add <eos> tag in predication case, size = curr_max_length\n",
    "    always right pad\n",
    "    \"\"\"\n",
    "    print(\"Adding <eos> to headline and padding\")\n",
    "    if is_input:\n",
    "        if len(list_idx) >= curr_max_length - 1:\n",
    "            return list_idx[:curr_max_length - 1]\n",
    "        else:\n",
    "            # space remaning add eos and empty tags\n",
    "            list_idx = list_idx + [eos_tag_location, ]\n",
    "            return padding(list_idx, curr_max_length - 1, False)\n",
    "    else:\n",
    "        # always add <eos>\n",
    "        if len(list_idx) == curr_max_length:\n",
    "            list_idx[-1] = eos_tag_location\n",
    "            return list_idx\n",
    "        else:\n",
    "            # space remaning add eos and empty tags\n",
    "            list_idx = list_idx + [eos_tag_location, ]\n",
    "            return padding(list_idx, curr_max_length, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def desc2idx(list_idx, curr_max_length):\n",
    "    \"\"\"\n",
    "    always left pad and eos tag to end\n",
    "    \"\"\"\n",
    "    #====== REVERSE THE DESC IDS ========\n",
    "    list_idx.reverse()\n",
    "    # padding to the left remain same and \n",
    "    # eos tag position also remain same,\n",
    "    # just description flipped\n",
    "    #===================================\n",
    "    # desc padded left\n",
    "    print(\"Adding <eos> to description and padding, reversing\")\n",
    "    list_idx = padding(list_idx, curr_max_length, True)\n",
    "    # eos tag add\n",
    "    list_idx = list_idx + [eos_tag_location, ]\n",
    "    return list_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2idx(sentence, is_headline, curr_max_length, is_input=True):\n",
    "    \"\"\"\n",
    "    given a sentence convert it to its ids\n",
    "    \"I like India\" => [12, 51, 102]\n",
    "    if words not present in vocab ignore them\n",
    "    is_input is only for headlines\n",
    "    \"\"\"\n",
    "    print(\"Converting sentences to vectors using word2ix obtained from model\")\n",
    "    list_idx = []\n",
    "    tokens = sentence.split(\" \")\n",
    "    count = 0\n",
    "    for each_token in tokens:\n",
    "        if each_token in word2idx:\n",
    "            list_idx.append(word2idx[each_token])\n",
    "        else:\n",
    "            #append unk token as original word not present in word2vec\n",
    "            list_idx.append(word2idx['<unk>'])\n",
    "        count = count + 1\n",
    "        if count >= curr_max_length:\n",
    "            break\n",
    "\n",
    "    if is_headline:\n",
    "        return headline2idx(list_idx, curr_max_length, is_input)\n",
    "    else:\n",
    "        return desc2idx(list_idx, curr_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_inputs(descriptions, headlines):\n",
    "    \"\"\"\n",
    "    convert input to suitable format\n",
    "    1.Left pad descriptions with <empty> tag\n",
    "    2.Add <eos> tag\n",
    "    3.Right padding with <empty> tag after (desc+headline)\n",
    "    4.input headline doesnot contain <eos> tag\n",
    "    5.expected/predicated headline contain <eos> tag\n",
    "    6.One hot endoing for expected output\n",
    "    \"\"\"\n",
    "    # length of headlines and descriptions should be equal\n",
    "    #assert len(descriptions) == len(headlines)\n",
    "\n",
    "    X, y = [], []\n",
    "    input_headline_idx = sentence2idx(headlines, True, max_len_head, True)\n",
    "    predicted_headline_idx = sentence2idx(headlines, True, max_len_head, False)\n",
    "    desc_idx = sentence2idx(descriptions, False, max_len_desc)\n",
    "    #print(\"Input headline length\",len(input_headline_idx))\n",
    "    #print(\"Predicted headline length\",len(predicted_headline_idx))\n",
    "    #print(\"Description length\",len(desc_idx))\n",
    "    # assert size checks\n",
    "    assert len(input_headline_idx) == max_len_head - 1\n",
    "    assert len(predicted_headline_idx) == max_len_head\n",
    "    assert len(desc_idx) == max_len_desc + 1\n",
    "\n",
    "    X.append(desc_idx + input_headline_idx)\n",
    "    y.append(predicted_headline_idx)\n",
    "        \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(\"Convert descs and headlines to numpy arrays\")\n",
    "    return X,headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexes_to_words(list_of_headline):\n",
    "    \"\"\"\n",
    "    indexes => words (for BLUE Score)\n",
    "    e.g. [2,0] => [\"I\",\"am\"] (idx2word defined dictionary used)\n",
    "    \"\"\"\n",
    "    list_of_word_headline = []\n",
    "    for each_headline in list_of_headline:\n",
    "        each_headline_words = []\n",
    "        for each_word in each_headline:\n",
    "            #Dont include <eos> and <empty> tags\n",
    "            if each_word in (empty_tag_location, eos_tag_location, unknown_tag_location):\n",
    "                continue\n",
    "            each_headline_words.append(idx2word[each_word])\n",
    "        list_of_word_headline.append(each_headline_words)            \n",
    "    return list_of_word_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_headline_end(word_index_list,current_predication_position):\n",
    "    \"\"\"\n",
    "    is headline ended checker\n",
    "    current_predication_position is 0 index based\n",
    "    \"\"\"\n",
    "    #print(\"Inside is_headline function\")\n",
    "    if (word_index_list is None) or (len(word_index_list)==0):\n",
    "        return False\n",
    "    if word_index_list[current_predication_position]==eos_tag_location or current_predication_position>=max_length:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word(predication,word_position_index,top_k,X,prev_layer_log_prob):\n",
    "    \"\"\"\n",
    "    Extract top k predications of given position\n",
    "    \"\"\"\n",
    "    #print(\"Inside process word function\")\n",
    "    #predication conttains only one element\n",
    "    #shape of predication (1,max_head_line_words,vocab_size)\n",
    "    predication = predication[0]\n",
    "    #predication (max_head_line_words,vocab_size)\n",
    "    predication_at_word_index = predication[word_position_index]\n",
    "    #http://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "    sorted_arg = predication_at_word_index.argsort()\n",
    "    top_probable_indexes = sorted_arg[::-1]\n",
    "    top_probabilities = np.take(predication_at_word_index,top_probable_indexes)\n",
    "    log_probabilities = np.log(top_probabilities)\n",
    "    #make sure elements doesnot contain -infinity\n",
    "    log_probabilities[log_probabilities == -inf] = -sys.maxsize - 1\n",
    "    #add prev layer probability\n",
    "    log_probabilities = log_probabilities + prev_layer_log_prob\n",
    "    assert len(log_probabilities)==len(top_probable_indexes)\n",
    "        \n",
    "    #add previous words ... preparation for next input\n",
    "    #offset calculate ... description + eos + headline till now\n",
    "    offset = max_len_desc+word_position_index+1\n",
    "    ans = []\n",
    "    count = 0 \n",
    "    for i,j in zip(log_probabilities, top_probable_indexes):\n",
    "        #check for word should not repeat in headline ... \n",
    "        #checking for last x words, where x = dont_repeat_word_in_last\n",
    "        if j in X[max_len_desc+1:offset][-dont_repeat_word_in_last:]:\n",
    "            continue\n",
    "        if (word_position_index < min_head_line_gen) and (j in [empty_tag_location, unknown_tag_location, eos_tag_location]):\n",
    "            continue\n",
    "            \n",
    "        next_input = np.concatenate((X[:offset], [j,]))\n",
    "        next_input = next_input.reshape((1,next_input.shape[0]))\n",
    "        #for the last time last word put at max_length + 1 position \n",
    "        #don't truncate that\n",
    "        if offset!=max_length:\n",
    "            next_input = sequence.pad_sequences(next_input, maxlen=max_length, value=empty_tag_location, padding='post', truncating='post')\n",
    "        next_input = next_input[0]\n",
    "        ans.append((i,next_input))\n",
    "        count = count + 1\n",
    "        if count>=top_k:\n",
    "            break\n",
    "    #[(prob,list_of_words_as_next_input),(prob2,list_of_words_as_next_input2),...]\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(X,top_k):\n",
    "    \"\"\"\n",
    "    1.Loop over max headline word allowed\n",
    "    2.predict word prob and select top k words for each position\n",
    "    3.select top probable combination uptil now for next round\n",
    "    \"\"\"\n",
    "    print(\"Inside beam search function\")\n",
    "    #contains [(log_p untill now, word_seq), (log_p2, word_seq2)]\n",
    "    prev_word_index_top_k = []\n",
    "    curr_word_index_top_k = []\n",
    "    done_with_pred = []\n",
    "    #1d => 2d array [1,2,3] => [[1,2,3]]\n",
    "    data = X.reshape((1,X.shape[0]))\n",
    "    #shape of predication (1,max_head_line_words,vocab_size)\n",
    "    predication = model.predict_proba(data,verbose=0)\n",
    "    #prev layer probability 1 => np.log(0)=0.0\n",
    "    prev_word_index_top_k = process_word(predication,0,top_k,X,0.0)\n",
    "        \n",
    "    #1st time its done above to fill prev word therefore started from 1\n",
    "    for i in range(1,max_len_head):\n",
    "        #i = represents current intrested layer ...\n",
    "        for j in range(len(prev_word_index_top_k)):\n",
    "            #j = each time loops for top k results ...\n",
    "            probability_now, current_intput = prev_word_index_top_k[j]\n",
    "            data = current_intput.reshape((1,current_intput.shape[0]))\n",
    "            predication = model.predict_proba(data,verbose=1)\n",
    "            next_top_k_for_curr_word = process_word(predication,i,top_k,current_intput,probability_now)\n",
    "            curr_word_index_top_k = curr_word_index_top_k + next_top_k_for_curr_word\n",
    "                \n",
    "        #sort new list, empty old, copy top k element to old, empty new\n",
    "        curr_word_index_top_k = sorted(curr_word_index_top_k,key=itemgetter(0),reverse=True)\n",
    "        prev_word_index_top_k_temp = curr_word_index_top_k[:top_k]\n",
    "        curr_word_index_top_k = []\n",
    "        prev_word_index_top_k = []\n",
    "        #if word predication eos ... put it done list ...\n",
    "        for each_proba, each_word_idx_list in prev_word_index_top_k_temp:\n",
    "            offset = max_len_desc+i+1\n",
    "            if is_headline_end(each_word_idx_list,offset):\n",
    "                done_with_pred.append((each_proba, each_word_idx_list))\n",
    "            else:\n",
    "                prev_word_index_top_k.append((each_proba,each_word_idx_list))\n",
    "            \n",
    "    #sort according to most probable\n",
    "    done_with_pred = sorted(done_with_pred,key=itemgetter(0),reverse=True)\n",
    "    done_with_pred = done_with_pred[:top_k]\n",
    "    return done_with_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(descriptions,headlines,top_k,seperator='#|#'):\n",
    "    \"\"\"\n",
    "    test on given description data file with empty headline ...\n",
    "    \"\"\"\n",
    "    #model.load_weights(model_weights_file_name)\n",
    "    #print (\"model weights loaded\")\n",
    "    print(\"Inside test function\")\n",
    "    X,Y = convert_inputs(descriptions, headlines)\n",
    "    X = X[0]\n",
    "    Y = Y[0]\n",
    "    assert X[max_len_desc]==eos_tag_location\n",
    "    #wipe up news headlines present and replace by empty tag ...            \n",
    "    X[max_len_desc+1:]=empty_tag_location\n",
    "    result = beam_search(X,top_k)\n",
    "    #take top most probable element\n",
    "    list_of_word_indexes = []\n",
    "    #for results in result:\n",
    "        #list_of_word_indexes.append(results[1])\n",
    "    list_of_word_indexes = result[0][1]\n",
    "    list_of_words = indexes_to_words([list_of_word_indexes])[0]\n",
    "    print(list_of_words)\n",
    "    headline = \" \".join(list_of_words)\n",
    "    return str(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "WORD = re.compile(r'\\w+')\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Apr/2018 15:20:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Apr/2018 15:20:22] \"GET /index/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside test function\n",
      "Converting sentences to vectors using word2ix obtained from model\n",
      "Adding <eos> to headline and padding\n",
      "Padding\n",
      "Converting sentences to vectors using word2ix obtained from model\n",
      "Adding <eos> to headline and padding\n",
      "Padding\n",
      "Converting sentences to vectors using word2ix obtained from model\n",
      "Adding <eos> to description and padding, reversing\n",
      "Padding\n",
      "Convert descs and headlines to numpy arrays\n",
      "Inside beam search function\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 1s 500ms/step\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 1s 500ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 700ms/step\n",
      "1/1 [==============================] - 1s 728ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 592ms/step\n",
      "1/1 [==============================] - 1s 596ms/step\n",
      "1/1 [==============================] - 1s 612ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 1s 500ms/step\n",
      "['Name', 'The', 'Trump', 'for', 'the', 'New', 'York', 'Times', 'in', 'The', 'Trump']\n",
      "Name The Trump for the New York Times in The Trump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2018 15:21:29] \"POST /result HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside test function\n",
      "Converting sentences to vectors using word2ix obtained from model\n",
      "Adding <eos> to headline and padding\n",
      "Converting sentences to vectors using word2ix obtained from model\n",
      "Adding <eos> to headline and padding\n",
      "Converting sentences to vectors using word2ix obtained from model\n",
      "Adding <eos> to description and padding, reversing\n",
      "Padding\n",
      "Convert descs and headlines to numpy arrays\n",
      "Inside beam search function\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 1s 548ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 1s 592ms/step\n",
      "1/1 [==============================] - 1s 604ms/step\n",
      "1/1 [==============================] - 1s 648ms/step\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "1/1 [==============================] - 1s 612ms/step\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 548ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "['firing', 'chaplain', 'House', 'over', 'Ryan', 'grill', 'Republicans', 'Some', 'The', 'Trump', 'is', 'the', 'New', 'York', 'on', 'Trump', 'The', 'the']\n",
      "firing chaplain House over Ryan grill Republicans Some The Trump is the New York on Trump The the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2018 15:24:57] \"POST /result HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, send_from_directory\n",
    "app = Flask(__name__)\n",
    "\n",
    "def get_generated_title(actual_headline,actual_body):\n",
    "    gen_title = test(actual_headline,actual_body,5)\n",
    "    print(gen_title)\n",
    "    #gen_title=\"I am the generated title\"\n",
    "    return(str(gen_title))\n",
    "\n",
    "def get_cosine_similarity(actual_headline,gen_title):\n",
    "    vector1 = text_to_vector(actual_headline)\n",
    "    vector2 = text_to_vector(gen_title)\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    return(cosine)\n",
    "\n",
    "@app.route('/')\n",
    "def landing_page():\n",
    "    return ('Welcome!!!!')\n",
    "\n",
    "@app.route('/index/')\n",
    "def index_page():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/result',methods = ['POST', 'GET'])\n",
    "def result_page():\n",
    "    if request.method == 'POST':\n",
    "        result = request.form\n",
    "        #cosine='99%'\n",
    "        actual_headline = request.form['newstitle']\n",
    "        actual_body = request.form['newsbody']\n",
    "        #print(actual_headline)\n",
    "        #print(actual_body)\n",
    "        gen_title = get_generated_title(actual_headline,actual_body)\n",
    "        cosine = get_cosine_similarity(actual_headline,gen_title)\n",
    "        return render_template(\"result.html\",result = result, cosine=cosine, gen_title = gen_title)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
